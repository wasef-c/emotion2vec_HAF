# Advanced Architecture Ablation Study
# Testing transformer, temporal CNN, and ConvFormer architectures for 70% WA target

study_name: "advanced_architecture_ablation"
wandb_project: "emotion2vec_advanced_ablation"

# Base configuration for all experiments
base_config:
  # Core training parameters
  num_epochs: 50
  batch_size: 20
  weight_decay: 1e-4
  lr_scheduler: "cosine"
  
  # Class weights (best performing from previous study)
  class_weights:
    neutral: 1.0
    happy: 2.5
    sad: 1.5
    anger: 1.5
  
  # Base setup - will be overridden per experiment
  use_curriculum_learning: false
  use_speaker_disentanglement: false
  difficulty_method: "original"
  curriculum_epochs: 20
  
  # Fast testing
  single_test_session: 5

# Experiment definitions
experiments:
  # ========================================
  # BASELINE - SIMPLE ARCHITECTURE
  # ========================================
  
  - id: "baseline_simple"
    name: "Baseline Simple"
    learning_rate: 3e-4
    classifier_config:
      architecture: "simple"
      hidden_dim: 256
      pooling: "mean"
      layer_norm: false
      dropout: 0.2
    description: "Baseline simple classifier for comparison"
    category: "baseline"

  - id: "baseline_simple_enhanced"
    name: "Baseline Simple Enhanced"
    learning_rate: 3e-4
    classifier_config:
      architecture: "simple"
      hidden_dim: 512
      pooling: "max_mean"
      layer_norm: true
      dropout: 0.2
    description: "Enhanced simple classifier with best previous improvements"
    category: "baseline"

  # ========================================
  # TRANSFORMER ARCHITECTURES
  # ========================================
  
  - id: "transformer_small"
    name: "Transformer Small"
    learning_rate: 1e-4
    classifier_config:
      architecture: "transformer"
      hidden_dim: 256
      dropout: 0.2
      num_heads: 8
      num_layers: 2
    description: "Small transformer with 2 layers"
    category: "transformer"

  - id: "transformer_medium"
    name: "Transformer Medium"
    learning_rate: 1e-4
    classifier_config:
      architecture: "transformer"
      hidden_dim: 256
      dropout: 0.2
      num_heads: 8
      num_layers: 4
    description: "Medium transformer with 4 layers"
    category: "transformer"

  - id: "transformer_large"
    name: "Transformer Large"
    learning_rate: 1e-4
    classifier_config:
      architecture: "transformer"
      hidden_dim: 512
      dropout: 0.2
      num_heads: 8
      num_layers: 4
    description: "Large transformer with 512 hidden dim"
    category: "transformer"

  # ========================================
  # TEMPORAL CNN ARCHITECTURES
  # ========================================
  
  - id: "temporal_cnn_basic"
    name: "Temporal CNN Basic"
    learning_rate: 3e-4
    classifier_config:
      architecture: "temporal_cnn"
      hidden_dim: 256
      dropout: 0.2
      kernel_sizes: [3, 5, 7, 9]
    description: "Basic temporal CNN with standard kernel sizes"
    category: "temporal_cnn"

  - id: "temporal_cnn_large"
    name: "Temporal CNN Large"
    learning_rate: 3e-4
    classifier_config:
      architecture: "temporal_cnn"
      hidden_dim: 512
      dropout: 0.2
      kernel_sizes: [3, 5, 7, 9]
    description: "Large temporal CNN with 512 hidden dim"
    category: "temporal_cnn"

  - id: "temporal_cnn_multiscale"
    name: "Temporal CNN Multiscale"
    learning_rate: 3e-4
    classifier_config:
      architecture: "temporal_cnn"
      hidden_dim: 256
      dropout: 0.2
      kernel_sizes: [3, 5, 7, 9, 11, 13]
    description: "Temporal CNN with more kernel sizes for multiscale"
    category: "temporal_cnn"

  # ========================================
  # CONVFORMER ARCHITECTURES
  # ========================================
  
  - id: "convformer_small"
    name: "ConvFormer Small"
    learning_rate: 1e-4
    classifier_config:
      architecture: "convformer"
      hidden_dim: 256
      dropout: 0.2
      num_heads: 8
      num_layers: 2
    description: "Small ConvFormer with 2 transformer layers"
    category: "convformer"

  - id: "convformer_medium"
    name: "ConvFormer Medium"
    learning_rate: 1e-4
    classifier_config:
      architecture: "convformer"
      hidden_dim: 512
      dropout: 0.2
      num_heads: 8
      num_layers: 2
    description: "Medium ConvFormer with 512 hidden dim"
    category: "convformer"

  - id: "convformer_large"
    name: "ConvFormer Large"
    learning_rate: 1e-4
    classifier_config:
      architecture: "convformer"
      hidden_dim: 512
      dropout: 0.2
      num_heads: 8
      num_layers: 4
    description: "Large ConvFormer with 4 transformer layers"
    category: "convformer"

  # ========================================
  # LEARNING RATE VARIATIONS
  # ========================================
  
  - id: "transformer_lr_5e4"
    name: "Transformer LR 5e-4"
    learning_rate: 5e-4
    classifier_config:
      architecture: "transformer"
      hidden_dim: 256
      dropout: 0.2
      num_heads: 8
      num_layers: 4
    description: "Transformer with higher learning rate"
    category: "lr_variation"

  - id: "temporal_cnn_lr_1e4"
    name: "Temporal CNN LR 1e-4"
    learning_rate: 1e-4
    classifier_config:
      architecture: "temporal_cnn"
      hidden_dim: 256
      dropout: 0.2
      kernel_sizes: [3, 5, 7, 9]
    description: "Temporal CNN with lower learning rate"
    category: "lr_variation"

  # ========================================
  # BEST COMBINATIONS
  # ========================================
  
  - id: "best_transformer"
    name: "Best Transformer"
    learning_rate: 1e-4
    classifier_config:
      architecture: "transformer"
      hidden_dim: 512
      dropout: 0.1
      num_heads: 8
      num_layers: 4
    description: "Best transformer configuration for 70% WA target"
    category: "best_combo"

  - id: "best_convformer"
    name: "Best ConvFormer"
    learning_rate: 1e-4
    classifier_config:
      architecture: "convformer"
      hidden_dim: 512
      dropout: 0.1
      num_heads: 8
      num_layers: 3
    description: "Best ConvFormer configuration for 70% WA target"
    category: "best_combo"

# Analysis settings
analysis:
  primary_metric: "iemocap_wa"
  secondary_metrics:
    - "iemocap_uar"
    - "cross_corpus_uar"
  
  research_questions:
    - "Which architecture performs best for emotion recognition?"
    - "Do transformers outperform CNNs for temporal emotion modeling?"
    - "Does ConvFormer hybrid approach provide best of both worlds?"
    - "What's the optimal hidden dimension for each architecture?"
    - "What learning rate works best for each architecture type?"
    - "Can we achieve 70% WA on IEMOCAP with these architectures?"

# Expected runtime
estimated_runtime:
  total_experiments: 16
  minutes_per_experiment: 45
  total_hours: 12

# Expected improvements
expected_results:
  baseline_wa: 0.60  # Current simple classifier
  target_wa: 0.70   # Goal with advanced architectures
  best_architectures:
    - transformer
    - convformer
    - temporal_cnn
  expected_improvements:
    - "Transformer: Better long-range temporal modeling"
    - "Temporal CNN: Efficient multiscale feature extraction"
    - "ConvFormer: Combines local and global temporal patterns"